{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ryan Bales (@ryanbales)<br>ryan@balesofdata.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner']) \n",
    "\n",
    "# Plotting Tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/2016_debates/\"\n",
    "transcription_data_path = \"{}{}\".format(data_path, \"transcripts/\")\n",
    "topic_count = 5\n",
    "words_per_topic = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(file_list):\n",
    "    data = []\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            transcript = json.load(f)\n",
    "            data.append(transcript[\"results\"][\"transcripts\"][0][\"transcript\"])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're using spacy for Lemmatization <br/> You'll need to run the following command to install the enlish package for spacy: python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Transcription Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"{}{}\".format(transcription_data_path, \"debate_1.mp3.json\"),\n",
    "    \"{}{}\".format(transcription_data_path, \"debate_2.mp3.json\"),\n",
    "    \"{}{}\".format(transcription_data_path, \"debate_3.mp3.json\"),\n",
    "    \"{}{}\".format(transcription_data_path, \"vp_debate.mp3.json\")\n",
    "]\n",
    "                       \n",
    "docs = get_text(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Documents and Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_docs_no_stops = []\n",
    "\n",
    "for doc in docs:\n",
    "    clean_doc = []\n",
    "    for word in gensim.utils.simple_preprocess(str(doc)):\n",
    "        if word not in nlp.Defaults.stop_words:\n",
    "            clean_doc.append(word)\n",
    "            \n",
    "    clean_docs_no_stops.append(clean_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_clean = lemmatization(clean_docs_no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dictionary and Corpus for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(text_data_clean)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're Ready to Build the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=topic_count, random_state=100, update_every=1, \n",
    "                                            chunksize=100, passes=10, alpha='auto', per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Topic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"say\" + 0.001*\"people\" + 0.001*\"want\" + 0.001*\"go\" + 0.001*\"country\" + 0.001*\"know\" + 0.001*\"ve\" + 0.001*\"think\" + 0.001*\"trump\" + 0.001*\"look\"'),\n",
       " (1,\n",
       "  '0.001*\"go\" + 0.001*\"say\" + 0.001*\"people\" + 0.001*\"know\" + 0.001*\"trump\" + 0.001*\"country\" + 0.001*\"question\" + 0.001*\"thing\" + 0.001*\"look\" + 0.001*\"ve\"'),\n",
       " (2,\n",
       "  '0.016*\"go\" + 0.013*\"say\" + 0.012*\"people\" + 0.011*\"want\" + 0.010*\"think\" + 0.010*\"country\" + 0.009*\"trump\" + 0.009*\"know\" + 0.009*\"ve\" + 0.007*\"look\"'),\n",
       " (3,\n",
       "  '0.001*\"go\" + 0.001*\"say\" + 0.001*\"want\" + 0.001*\"people\" + 0.001*\"country\" + 0.001*\"know\" + 0.001*\"trump\" + 0.001*\"ve\" + 0.001*\"think\" + 0.001*\"work\"'),\n",
       " (4,\n",
       "  '0.001*\"say\" + 0.001*\"go\" + 0.001*\"know\" + 0.001*\"people\" + 0.001*\"ve\" + 0.001*\"want\" + 0.001*\"trump\" + 0.001*\"thing\" + 0.001*\"american\" + 0.001*\"think\"')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_topics=topic_count, num_words=words_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8888/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rbales/.virtualenvs/audio-data-insights/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n",
      "127.0.0.1 - - [10/Jan/2019 23:31:47] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jan/2019 23:31:47] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jan/2019 23:31:47] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jan/2019 23:31:47] \"GET /LDAvis.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Jan/2019 23:31:48] code 404, message Not Found\n",
      "127.0.0.1 - - [10/Jan/2019 23:31:48] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [11/Jan/2019 09:28:03] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2019 09:28:03] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2019 09:28:03] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2019 09:28:03] \"GET /LDAvis.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jan/2019 09:05:21] code 404, message Not Found\n",
      "127.0.0.1 - - [12/Jan/2019 09:05:21] \"GET /Cleveland_Museum_of_Natural_History_1023/wp-content/uploads/cuyahoga-arts-and-culture.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [12/Jan/2019 09:05:22] code 400, message Bad request syntax ('\\x16\\x03\\x01\\x02\\x00\\x01\\x00\\x01ü\\x03\\x03Ò\\x7f\\x7f¹\\x06gÍ8ËQcGÏÝ,b·¢f\\x95R©m<§\\x8c\\x8bÿ¯·\\x15° Mª9\\x15\\x10lÞÙ.\\tzC\\x04\"PXI®Ë\\x9f\\x10\\x9e6¾\\x1aB\\x1d½\\x9cõ29\\x00\"zz\\x13\\x01\\x13\\x02\\x13\\x03À+À/À,À0Ì©Ì¨À\\x13À\\x14\\x00\\x9c\\x00\\x9d\\x00/\\x005\\x00')\n",
      "127.0.0.1 - - [12/Jan/2019 09:05:22] \"\u0016\u0003\u0001\u0002\u0000\u0001\u0000\u0001ü\u0003\u0003Ò¹\u0006gÍ8ËQcGÏÝ,b·¢fR©m<§ÿ¯·\u0015° Mª9\u0015\u0010lÞÙ.\tzC\u0004\"PXI®Ë\u00106¾\u001aB\u001d",
      "½õ29\u0000\"zz\u0013\u0001\u0013\u0002\u0013\u0003À+À/À,À0Ì©Ì¨À\u0013À\u0014\u0000\u0000\u0000/\u00005\u0000\" 400 -\n",
      "127.0.0.1 - - [12/Jan/2019 09:05:22] code 400, message Bad HTTP/0.9 request type ('\\x16\\x03\\x01\\x00Ã\\x01\\x00\\x00¿\\x03\\x03\\x7fT\\x08<ù.Ø¢\\x86QUX\\x9c³1È\\x03:\\x071ÎFè')\n",
      "127.0.0.1 - - [12/Jan/2019 09:05:22] \"\u0016\u0003\u0001\u0000Ã\u0001\u0000\u0000¿\u0003\u0003<ù.Ø¢QUX³1È\u0003:\u00071ÎFè |ªÆúmY\" 400 -\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.show(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
